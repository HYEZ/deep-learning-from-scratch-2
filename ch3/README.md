## 통계 기반 기법의 문제점
- 데이터를 한번에 처리 (`배치 학습`)
- SVD 경우 수행시간이 __n**3__ => 데이터가 클 경우 현실적으로 불가능
- `추론 기반 기법은` 데이터를 나눠서 처리하는 `미니배치 학습`을 하기 때문에 계산량이 커도 가능함

***

## 추론 기반 기법(신경망)
- 주변 단어(`맥락`)가 주어졌을 때 무슨 단어가 들어가는지를 추론
- 추론 문제를 반복해서 풀면서 단어의 출현 패턴을 학습함
- 맥락을 입력하면 모델은 출현할 수 있는 각 단어의 출현 확률을 출력한다.
- 말뭉치를 사용해 모델이 올바른 추측을 내놓도록 학습함
- 학습의 결과로 단어의 분산 표현을 얻음
- `통계 기반 기법`처럼 __단어의 의미는 주변 단어에 의해 형성된다는 `분포 가설`에 기초__

### 신경망에서 단어 처리 
- 단어를 `고정 길이 벡터`로 변환
	- one-hot vector 주로 사용
	- 입력층은 뉴런의 수를 `고정`할 수 있음 (feature 수를 어휘의 수만큼)

*** 

## CBOW
- 맥락(주변 단어)으로 부터 타깃(중앙 단어)를 추측
	- 입력 : 맥락(주변 단어들의 목록)을 one-hot vector로 표현
- `완전연결계층(fully-connected layer)`이 처리
- input layer
	- 맥락의 단어 수(N) = 입력 벡터 수(N)
- hidden layer
	- 은닉충의 뉴런은 입력층의 완전연결계층에 의해 변환된 값
	- 입력층이 여러개면 전체를 `평균`함
		- 입력: (Nx7), W: (7x3) => hidden layer: __(Nx3)__
		- But, we should change (Nx3) into (1x3)
		- 그래서 그 평균을 구함
- output layer
	- 출력층의 뉴런 하나하나는 __각각의 단어에 대응됨__
	- 출력층의 뉴런은 각 단어의 `점수(score)`임 (값이 높을 수록 대응 단어의 출현 확률 높음)
		- 확률로 해석하기 위해 `소프트맥스`함수 적용시키면 됨
- 가중치
	- input=>hidden(W_in)
		- 각 해당 단어의 `분산 표현`이 담김
		- 학습을 진행할수록 맥락에서 출현하는 단어를 잘 추측하는 방향으로 이 분산표현들이 갱신될 것임
		- 이렇게 해서 얻은 벡터엔 `단어의 의미`도 잘 녹아들어 있다!
		- __은닉층의 뉴런 수를 입력 층의 뉴런 수보다 적게 해야함__(단어 예측에 필요한 정보를 간결하게 담고, 밀집벡터 표현을 얻기 위해)

