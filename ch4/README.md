## 3장에서 만든 CBOW 모델 문제점
- __다루는 어휘수가 많을 때(거대한 말뭉치) 문제가 발생__
1. 입력층의 원핫벡터와 가중치행렬 W_in의 곱 계산 => `Embedding 계층`으로 해결
2. 은닝층과 가중치 행렬 W_out의 곱 및 Softmax 계층의 계산 => `negative sampling`으로 해결

***

## Embedding 계층
- 사실 입력층과 W_in의 행렬곱은 필요가 없음(`원핫벡터`라 특정 행만 추출하기 때문)
- 그래서 단어 ID에 해당하는 행을 추출하는 계층을 만듬 = Embedding 계층
- Embedding 계층에 단어 임베딩(분산표현)을 저장하는 것

### 단어 인베딩
- NLP에서 단어의 밀집벡터 표현을 `단어 임베딩` 혹은 `분산 표현`이라고 한다.


***

## Negative Sampling
- 은닉층과 W_out 행렬곱, 소프트맥스 계층의 계산 속도 해결
	- 어휘가 많아지면 행렬곱 계산량 증가, 소프트맥스도 마찬가지로 계산량(exp 계산) 증가함
- 어휘가 아무리 많아도 계산량을 낮은 수준에서 일정하게 억제 가능
- __`다중 분류(multi-classification)`를 `이진 분류(binary classification)`로 근사하는 것__
	- ex) 맥락이 'you'와 'goodbye'일 때 타깃 단어는 'say'입니까? => O/X
- 출력층 뉴런은 1개만 필요함
- 은닉층과 W_out의 내적은 타깃에 해당하는 열벡터만 추출, 그 열벡터와 은닉층 뉴런과의 내적을 계산하면 끝남
- __모든 단어가 아닌 타깃 단어에만 집중해서 점수를 계산한다!__

### 시그모이드 함수와 교차 엔트로피 오차(CEE)
- `이진 분류(binary clasification)` 신경망에서 가장 흔하게 사용하는 조합이다!
	- 보통 `다중 분류` 모델은 `소프트맥스`와 `CEE` 사용
- `시그모이드`함수의 출력은 `확률`로 해석 가능(0~1)
- 역전파는 y-t (출력과 정답의 차이)
