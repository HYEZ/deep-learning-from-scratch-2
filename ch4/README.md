## 3장에서 만든 CBOW 모델 문제점
- __다루는 어휘수가 많을 때(거대한 말뭉치) 문제가 발생__
1. 입력층의 원핫벡터와 가중치행렬 W_in의 곱 계산 => `Embedding 계층`으로 해결
2. 은닝층과 가중치 행렬 W_out의 곱 및 Softmax 계층의 계산 => `negative sampling`으로 해결

***

## Embedding 계층
- 사실 입력층과 W_in의 행렬곱은 필요가 없음(`원핫벡터`라 특정 행만 추출하기 때문)
- 그래서 단어 ID에 해당하는 행을 추출하는 계층을 만듬 = Embedding 계층
- Embedding 계층에 단어 임베딩(분산표현)을 저장하는 것

### 단어 인베딩
- NLP에서 단어의 밀집벡터 표현을 `단어 임베딩` 혹은 `분산 표현`이라고 한다.

