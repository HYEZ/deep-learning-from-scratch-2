## 3장에서 만든 CBOW 모델 문제점
- __다루는 어휘수가 많을 때(거대한 말뭉치) 문제가 발생__
1. 입력층의 원핫벡터와 가중치행렬 W_in의 곱 계산 => `Embedding 계층`으로 해결
2. 은닝층과 가중치 행렬 W_out의 곱 및 Softmax 계층의 계산 => `negative sampling`으로 해결

***

## Embedding 계층
- 사실 입력층과 W_in의 행렬곱은 필요가 없음(`원핫벡터`라 특정 행만 추출하기 때문)
- 그래서 단어 ID에 해당하는 행을 추출하는 계층을 만듬 = Embedding 계층
- Embedding 계층에 단어 임베딩(분산표현)을 저장하는 것

### 단어 인베딩
- NLP에서 단어의 밀집벡터 표현을 `단어 임베딩` 혹은 `분산 표현`이라고 한다.


***

## Negative Sampling
- 은닉층과 W_out 행렬곱, 소프트맥스 계층의 계산 속도 해결
	- 어휘가 많아지면 행렬곱 계산량 증가, 소프트맥스도 마찬가지로 계산량(exp 계산) 증가함
- 어휘가 아무리 많아도 계산량을 낮은 수준에서 일정하게 억제 가능

### 다중 분류(multi-classification)를 이진 분류(binary classification)로 근사하는 것
	- ex) 맥락이 'you'와 'goodbye'일 때 타깃 단어는 'say'입니까? => O/X
- 출력층 뉴런은 1개만 필요함
- 은닉층과 W_out의 내적은 타깃에 해당하는 열벡터만 추출, 그 열벡터와 은닉층 뉴런과의 내적을 계산하면 끝남
- __모든 단어가 아닌 타깃 단어에만 집중해서 점수를 계산한다!__

### 시그모이드 함수와 교차 엔트로피 오차(CEE)
- `이진 분류(binary clasification)` 신경망에서 가장 흔하게 사용하는 조합이다!
	- 보통 `다중 분류` 모델은 `소프트맥스`와 `CEE` 사용
- `시그모이드`함수의 출력은 `확률`로 해석 가능(0~1)
- 역전파는 y-t (출력과 정답의 차이)

### Negative sampling 이란?
- 위의 신경망은 긍정적인 단어(타겟)에 대해서만 학습, __부정적인 단어(타겟 이외의 단어)에 대해선 학습 안함__
- 우리가 하고싶은 것은 긍정적인 예(타겟)에 대해서는 Sigmoid 출력을 1에 가깝게, 부정적인 예(타겟 이외)에 대해서는 Sigmoid 출력을  0으로 만드는 것이다.
- 그러므로 이런 결과를 만들어 주는 `가중치`가 필요하다.
- 그렇다고 모든 부정적인 결과 학습하는건 X (어휘수 증가하기 때문)
- __따라서 부정적인 예를 `샘플링`하자!!__ => `negative sampling`
- __`negative sampling`은 `긍정적인 예`를 타깃으로 한 손실을 구함(레이블:1). 동시에 `부정적 예`를 샘플링해서 그 부정적 예에 대한 손실도 구함(레이블:0)__
- 그리고 각각의 손실을 더한 값이 `최종 손실`이다.

### Negative sampling의 샘플링 기법
- 말뭉치의 `통계 데이터`를 기초로 샘플링하자!
- 즉, 자주 등장하는 단어를 많이 추출하고, 적게 등장하는 단어를 적게 추출하는 것!
- 먼저 말뭉치에서 각 단어의 출현 횟수를 구해 `확률분포`로 나타내고, 그 확률분포대로 단어 샘플링하면 됨
- 단, 계산량 문제 때문에 적은수(5, 10 등)로 샘플링 수를 한정해야함
- word2vec은 확률분포에 `0.75`를 제곱하라고 권고함
	- 출현 확률이 낮은 단어를 _버리지 않기 위해서_

### 참고
- unigram: 하나의 (연속된) 단어
- bigram: 2개의 연속된 단어
- trigram: 3개의 연속된 단어






