## 신경망 종류
### 피드포워드(feed forward)
- 흐름이 `단방향`인 신경망
	- 입력신호가 다음층으로 전달되고 또 다음층으로 전달되고... 이런식으로 한방향으로 신호가 전달됨
- `시계열 데이터(time series data`를 잘 다루지 못한다.
	- 시계열 데이터의 성질(패턴)을 충분히 학습할 수 없다.

### 순환신경망(RNN)
- `시계열 데이터` 학습 가능

***

## 확률과 언어 모델
### 확률
- word2vec(CBOW, skip-gram 등)의 원래 목적은 context로부터 target을 정확히 추측하는 것!
- 근데 이 목적을 위해 학습을 진행하면 부산물로 단어의 의미가 인코딩된 `단어의 분산 표현`을 얻을 수 있음
- 그럼 본래 목적인 _'맥락으로부터 타깃을 추측하는 것'_ 은 어디에 이용할까? 
	- __P(w2 | w1, w3)은 실용적인 쓰임이 있을까? => `언어 모델` 등장__

### 언어 모델(Language Model)
1. 단어 나열에 확률을 부여함
	- 특정한 `시퀀스`에 대해서, 그 시퀀스가 일어날 가능성이 어느 정도인지(얼마나 자연스러운 단어 순서인지)를 확률로 평가함
2. 새로운 문장을 생성하는 용도
	- 언어 모델은 단어 순서의 자연스러움을 확률적으로 평가할 수 있으므로, 그 확률 분포에 따라 다음으로 적합한 단어를 `샘플링`할 수 있음
	- [7장. RNN을 사용한 문장 생성](https://github.com/HYEZ/deep-learning-from-scratch-2/tree/master/ch7)

#### 언어 모델을 수식으로 표현하기
- 동시 확률: 여러 사건이 동시에 일어나는 확률 => `곱셈정리`에 의해 `사후 확률의 총곱`으로 표현 가능
- 확률의 곱셈정리: P(A, B) = P(A|B)P(B)
	- A, B가 모두 일어날 확률은 B가 일어난 확률과 B가 일어난 후 A가 일어날 확률을 곱한 값과 같다.
	- A, B 중 어느 것을 _사후 확률의 조건_ 으로 할 건지에 대해 __2가지 방법이 존재__
- 단어가 w1, ..., wm 순서로 출현할 확률은 동시에 발생할 확률이므로 `동시 확률`으로 표현이 가능함
	- P(w1, w2, w3, ..., wm) = P(wm|w1,...,wm-1)P(wm-1|w1,...,wm-2)...P(w3|w1,w2)P(w2|w1)P(w1)
	- 단, 이 사후 확률은 타깃 단어보다 왼쪽에 있는 모든 단어를 맥락(조건)으로 했을 때의 확률이다!
	- 참고)
		- P(w1, ...,wt, ..., wm) = P(wm|w1,...,wm-1)P(wm-1|w1,...,wm-2)...P(w3|w1,w2)P(w2|w1)P(w1)
		- P(wt|w1, ..., wt-1)을 나타내는 모델은 `조건부 언어 모델(Conditional Language Model)`이라고 함
			- 이 확률을 계산할 수 있으면 우리는 언어모델의 동시 확률인 P(w1, ..., wm)를 구할 수 있음

#### CBOW 모델을 언어 모델로?
- word2vec의 CBOW 모델을 (억지로) 언어 모델에 적용
	- 맥락의 크기를 특정 값으로 한정하여 근사적으로 나타낸다.
- `마르코브 연쇄(Markov Chain)` or `마르코브 모델(Markov Model)`
	- 미래 상태가 현재 상태에만 의존해 결정되는 것
	- 이 사상의 확률이 그 직전 N개에만 의존할 때 `N층 마르코브 연쇄`라고 한다.
	- ex) 직전 2개의 단어에만 의존해서 다음 단어가 정해지는 모델 = 2층 마르코브 연쇄
- CBOW 모델 문제점
	- 맥락 수가 고정되어 있음 => 맥락이 아닌 부분은 기억 X
	- __맥락 안의 단어 순서는 무시됨__ (bag-of-words: 순서 대신 분포를 사용함)
	- => `신경 확률론적 언어 모델`: 단어 순서를 고려하기 위해 맥락의 단어 벡터를 은닉충에서 연결하자!
		- B/ 맥락의 크기에 비례해 가중치 매개변수도 늘어남
	- => `RNN`으로 해결
		- RNN은 맥락이 아무리 길더라도 그 맥락의 정보를 기억함

#### 참고
- word2vec은 단어의 분산 표현을 얻기 위한 것, 이를 언어 모델로 사용하는 경우는 잘 없음
- 이 책에선 word2vec의 CBOW모델을 억지로 언어모델에 적용함
***




