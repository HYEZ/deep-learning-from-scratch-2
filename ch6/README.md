## RNN의 문제점
- 시계열 데이터의 `장기 의존 관계`를 학습하기 어려움
	- `BPTT`에서 `기울기 소실(vanishing gradients)` or `기울기 폭발(exploding gradients)` 때문
- RNN 특징 : 이전 시각의 `은닉 상태`를 이용함
- 현재 단순한 RNN 계층은 시간을 거슬러 올라갈수록 기울기가 작아지거나(기울기 소실) 혹은 커질 수 있음(기울기 폭발)
	- 기울기 소실 : 기울기가 일정 수준 이하로 작아지면 가중치 매개변수가 갱신X
		- `게이트`가 추가된 RNN이 해결 (`LSTM`, `GRU` 등)
	- 기울기 폭발 : overflow일으킴 => NaN
		- `기울기 클리핑(gradients clipping)`이 해결
- 기울기 소실 또는 기울기 폭발의 원인
	- tanh 미분
		- x가 0으로부터 멀어질수록 기울기가 작아짐 => 기울기가 tanh 노드를 지날때마다 값은 계속 작아진다.
			- tanh 함수를 T번 통과하면 기울기도 T번 반복해서 작아짐
	- Matmul(행렬곱)
		- 행렬 곱셈에서는 역전파시 매번 똑같은 가중치가 사용됨
		- 행렬 Wh를 T번 반복해서 곱했기 때문에 기울기 소실 또는 기울기 폭발 현상이 일어남
		- Wh가 스칼라라면 Wh가 1보다 크면 지수적으로 증가, 1보다 작으면 지수적으로 감소함
		- Wh가 행렬이라면 행렬의 `특잇값(Singular Value)`이 척도가 됨
			- 특잇값 : 데이터가 얼마나 퍼져있는지
			- 특잇값(여러 특잇값 중 최댓값)이 1보다 큰지 여부를 봄

***

## LSTM(Long Short-Term Memory)
- `기억 셀(memory cell)` 도입 : LSTM 전용 기억 메커니즘
	- 데이터를 자기 자신으로만(LSTM 계층 내에서만) 주고받음
	- LSTM 계층 내에서만 완결되고 다른 계층으로 출력 안 함
	- LSTM의 은닉상태 h는 다른 계층(위쪽)으로 출력함
	- ct: 과거로부터 시각 t까지의 모든 정보를 기억함
		- ht = tanh(ct)
- `게이트(gate)`
	- 데이터의 흐름을 제어
	- 열기/닫기, 어느 정도 열지 조절(데이터의 양)
		- 어느 정도 : `열림 상태(openness)`
			- 범위 : 0.0\~1.0 (시그모이드 함수 이용)
	- __'게이트를 얼마나 열까'라는 것도 데이터로부터 자동으로 학습함__
		- 게이트 전용 가중치 매개변수 필요

### output 게이트
- tanh(ct)에 게이트를 적용
- tanh(ct)의 각 원소가 '다음 시각의 은닉 상태에 얼마나 중요한가'를 조정함
- 즉, 은닉 상태 ht의 출력을 담당함
- `시그모이드 함수`(0\~1) : 데이터를 얼마나 통과시킬지를 정하는 비율로 사용
- `tanh`(-1\~1) : 인코딩된 정보의 강약을 표시

### forget 게이트
- `기억 셀`에 '무엇을 잊을까'를 명확하게 지시
- c(t-1)의 기억 중에서 불필요한 기억을 잊게 해주는 게이트

### 새로운 기억 셀
- 새로 기억해야 할 정보를 기억셀에 추가해야 함
- tanh 노드가 계산한 결과가 이전 시각의 기억 셀 c(t-1)에 더해짐
- tanh 노드는 게이트가 아니다. 새로운 정보를 기억 셀에 추가하는 것이 목적임
- 따라서 시그모이드가 아닌 tanh 사용함

### input 게이트
- 새로운 기억이 추가되는 정보로써의 가치가 얼마나 큰지를 판단

