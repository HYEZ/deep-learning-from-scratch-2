## RNN의 문제점
- 시계열 데이터의 `장기 의존 관계`를 학습하기 어려움
	- `BPTT`에서 `기울기 소실(vanishing gradients)` or `기울기 폭발(exploding gradients)` 때문
- RNN 특징 : 이전 시각의 `은닉 상태`를 이용함
- 현재 단순한 RNN 계층은 시간을 거슬러 올라갈수록 기울기가 작아지거나(기울기 소실) 혹은 커질 수 있음(기울기 폭발)
	- 기울기 소실 : 기울기가 일정 수준 이하로 작아지면 가중치 매개변수가 갱신X
	- 기울기 폭발 : overflow일으킴 => NaN
- 기울기 소실 또는 기울기 폭발의 원인
	- tanh 미분
		- x가 0으로부터 멀어질수록 기울기가 작아짐 => 기울기가 tanh 노드를 지날때마다 값은 계속 작아진다.
			- tanh 함수를 T번 통과하면 기울기도 T번 반복해서 작아짐
	- Matmul(행렬곱)
		- 행렬 곱셈에서는 역전파시 매번 똑같은 가중치가 사용됨
		- 행렬 Wh를 T번 반복해서 곱했기 때문에 기울기 소실 또는 기울기 폭발 현상이 일어남
		- Wh가 스칼라라면 Wh가 1보다 크면 지수적으로 증가, 1보다 작으면 지수적으로 감소함
		- Wh가 행렬이라면 행렬의 `특잇값(Singular Value)`이 척도가 됨
			- 특잇값 : 데이터가 얼마나 퍼져있는지
			- 특잇값(여러 특잇값 중 최댓값)이 1보다 큰지 여부를 봄

### 기울기 클리핑(gradients clipping)
- 기울기 폭발 해결