## 언어 모델 문장 생성 방법
1. 확률이 가장 높은 단어 => 결정적임
2. 확률적으로 선택(확률 분포에 따라) => 매번 결과가 달라질 수 있음


***

## seq2seq 원리
- Encoder-Decoder 모델이라고도 함
- Encoder
	- 마지막 은닉 상태 h에 입력 문장을 번역하는 데 필요한 정보가 들어감
	- 은닉 상태 h는 `고정 길이 벡터`이므로 결국 임의 길이의 문장을 고정 길이 벡터로 변환하는 것!
- Decoder
	- 문장 생성 모델을 이용
	- __LSTM 계층이 벡터 h를 입력받는다는 점만 다름__
	- <eos>를 decoder에 '시작/종료'를 알리는 구분자로 사용

### toy problem(더하기)
- `문자 단위 분할` => 가변 길이 시계열 데이터 => 미니배치 처리시 뭔가 해줘야함
	- `패딩` 사용
		- 패딩 전용 처리 필요함 (decoder에 입력된 정보가 패딩이라면 손실 결과에 반영 X)

### seq2seq 학습 속도 개선
1. 입력 데이터 반전 (Reverse)
	- 기울기 전파가 원활해짐
2. 엿보기 (Peeky)


