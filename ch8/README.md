## seq2seq 문제점
- Encoder 출력 : `고정 길이 벡터`(항상 같은 길이의 벡터로 변환한다.)
	- 문장의 길이에 상관없이 항상 같은 길이의 벡터에 밀어 넣어야 함

### Encoder 개선
- Encoder의 출력 길이를 입력 문장의 길이에 따라 바꿔주자
	- __각 시각(단어)의 은닉 상태 벡터를 모두 이용하자 => 벡터의 집합(행렬)을 출력함__

### Decoder 개선 1
- Encoder 개선에 따라 각 단어의 은닉 상태 벡터를 입력으로 받음
- '입력과 출력의 여러 단어 중 _어떤 단어끼리 서로 관련되어 있는가_'라는 `대응 관계`를 seq2seq에 학습시킬 수는 없을까?
	- `alignment`: 단어의 대응관계를 나타내는 정보 (cat-고양이)
	- `어텐션`은 alignment를 seq2seq에 자동으로 도입함 (이전에는 수작업)
- 우리는 필요한 정보에만 주목하여 그 정보로부터 시계열 변환을 수행하는 것이 목표! (이 구조를 어텐션이라고 한다!)

#### alignment 추출
- Decoder에 입력된 단어와 대응관계인 단어의 벡터를 hs에서 추출한다!
- But, 선택하는 작업은 미분할 수 없음 => `오차역전파법` 사용 불가
- 그럼 선택 작업을 미분 가능한 연산으로 대체 가능할까? => 하나를 선택하는 것이 아니라 __모든 것을 선택하고 각 단어의 중요도를 나타내는 가중치를 별도로 계산하도록 함__
- 각 단어와 가중치를 weighted sum하여 원하는 벡터를 얻는다.
	- 이것을 `맥락 벡터`라고 함

### Decoder 개선 2
- 각 단어의 중요도를 나타내는 가중치를 어떻게 구해야 할까? => 데이터로부터 자동으로 학습하도록
- LSTM 계층의 은닉 상태 벡터 h가 hs의 각 단어와 얼마나 비슷한가를 수치로 나타내는 것
- 벡터의 내적 이용 (두 벡터가 얼마나 같은 방향을 향하고 있는가)
- 벡터의 내적으로 유사도 구하고 소프트맥스 함수 등으로 정규화함

### Decoder 개선 3
- WeightSum 계층과 AttentionWeight 계층을 하나로 구현 (Attention 계층)


***


## 어텐션(Attention)
